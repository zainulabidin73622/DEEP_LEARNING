{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainulabidin73622/DEEP_LEARNING/blob/main/EXP%235%20vanishing_gradient_sigmoid_relu_120325.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhcSoTKtijrZ",
        "outputId": "1bb8e550-db2a-47dd-eac7-6e4ecadd0ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Network:\n",
            "\n",
            "Forward Pass (Sigmoid):\n",
            "Layer 1 Output (a1): 0.6225\n",
            "Layer 2 Output (a2): 0.5772\n",
            "Predicted Output (y_pred): 0.5717\n",
            "Loss: 0.0917\n",
            "\n",
            "Backward Pass (Gradients for Sigmoid):\n",
            "Gradient w.r.t. w3: -0.0605\n",
            "Gradient w.r.t. w2: -0.0080\n",
            "Gradient w.r.t. w1: -0.0015\n",
            "\n",
            "ReLU Network:\n",
            "\n",
            "Forward Pass (ReLU):\n",
            "Layer 1 Output (a1): 0.5000\n",
            "Layer 2 Output (a2): 0.2500\n",
            "Predicted Output (y_pred): 0.1250\n",
            "Loss: 0.3828\n",
            "\n",
            "Backward Pass (Gradients for ReLU):\n",
            "Gradient w.r.t. w3: -0.2188\n",
            "Gradient w.r.t. w2: -0.2188\n",
            "Gradient w.r.t. w1: -0.2188\n",
            "\n",
            "Comparison:\n",
            "Sigmoid Gradients:\n",
            "Gradient w.r.t. w3: -0.0605\n",
            "Gradient w.r.t. w2: -0.0080\n",
            "Gradient w.r.t. w1: -0.0015\n",
            "\n",
            "ReLU Gradients:\n",
            "Gradient w.r.t. w3: -0.2188\n",
            "Gradient w.r.t. w2: -0.2188\n",
            "Gradient w.r.t. w1: -0.2188\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Define the ReLU function and its derivative\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Initialize weights and biases\n",
        "w1, b1 = 0.5, 0\n",
        "w2, b2 = 0.5, 0\n",
        "w3, b3 = 0.5, 0\n",
        "\n",
        "# Input and true output\n",
        "x = 1\n",
        "y = 1\n",
        "\n",
        "# Sigmoid Network\n",
        "print(\"Sigmoid Network:\")\n",
        "\n",
        "# Forward pass (Sigmoid)\n",
        "z1_sigmoid = w1 * x + b1\n",
        "a1_sigmoid = sigmoid(z1_sigmoid)\n",
        "\n",
        "z2_sigmoid = w2 * a1_sigmoid + b2\n",
        "a2_sigmoid = sigmoid(z2_sigmoid)\n",
        "\n",
        "z3_sigmoid = w3 * a2_sigmoid + b3\n",
        "y_pred_sigmoid = sigmoid(z3_sigmoid)\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "loss_sigmoid = 0.5 * (y - y_pred_sigmoid) ** 2\n",
        "\n",
        "# Backward pass (Gradient calculation for Sigmoid)\n",
        "# Output layer\n",
        "dL_dy_pred_sigmoid = -(y - y_pred_sigmoid)\n",
        "dy_pred_dz3_sigmoid = sigmoid_derivative(z3_sigmoid)\n",
        "dL_dw3_sigmoid = dL_dy_pred_sigmoid * dy_pred_dz3_sigmoid * a2_sigmoid\n",
        "\n",
        "# Layer 2\n",
        "dL_da2_sigmoid = dL_dy_pred_sigmoid * dy_pred_dz3_sigmoid * w3\n",
        "da2_dz2_sigmoid = sigmoid_derivative(z2_sigmoid)\n",
        "dL_dw2_sigmoid = dL_da2_sigmoid * da2_dz2_sigmoid * a1_sigmoid\n",
        "\n",
        "# Layer 1\n",
        "dL_da1_sigmoid = dL_da2_sigmoid * da2_dz2_sigmoid * w2\n",
        "da1_dz1_sigmoid = sigmoid_derivative(z1_sigmoid)\n",
        "dL_dw1_sigmoid = dL_da1_sigmoid * da1_dz1_sigmoid * x\n",
        "\n",
        "# Print results for Sigmoid\n",
        "print(\"\\nForward Pass (Sigmoid):\")\n",
        "print(f\"Layer 1 Output (a1): {a1_sigmoid:.4f}\")\n",
        "print(f\"Layer 2 Output (a2): {a2_sigmoid:.4f}\")\n",
        "print(f\"Predicted Output (y_pred): {y_pred_sigmoid:.4f}\")\n",
        "print(f\"Loss: {loss_sigmoid:.4f}\")\n",
        "\n",
        "print(\"\\nBackward Pass (Gradients for Sigmoid):\")\n",
        "print(f\"Gradient w.r.t. w3: {dL_dw3_sigmoid:.4f}\")\n",
        "print(f\"Gradient w.r.t. w2: {dL_dw2_sigmoid:.4f}\")\n",
        "print(f\"Gradient w.r.t. w1: {dL_dw1_sigmoid:.4f}\")\n",
        "\n",
        "# ReLU Network\n",
        "print(\"\\nReLU Network:\")\n",
        "\n",
        "# Forward pass (ReLU)\n",
        "z1_relu = w1 * x + b1\n",
        "a1_relu = relu(z1_relu)\n",
        "\n",
        "z2_relu = w2 * a1_relu + b2\n",
        "a2_relu = relu(z2_relu)\n",
        "\n",
        "z3_relu = w3 * a2_relu + b3\n",
        "y_pred_relu = relu(z3_relu)\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "loss_relu = 0.5 * (y - y_pred_relu) ** 2\n",
        "\n",
        "# Backward pass (Gradient calculation for ReLU)\n",
        "# Output layer\n",
        "dL_dy_pred_relu = -(y - y_pred_relu)\n",
        "dy_pred_dz3_relu = relu_derivative(z3_relu)\n",
        "dL_dw3_relu = dL_dy_pred_relu * dy_pred_dz3_relu * a2_relu\n",
        "\n",
        "# Layer 2\n",
        "dL_da2_relu = dL_dy_pred_relu * dy_pred_dz3_relu * w3\n",
        "da2_dz2_relu = relu_derivative(z2_relu)\n",
        "dL_dw2_relu = dL_da2_relu * da2_dz2_relu * a1_relu\n",
        "\n",
        "# Layer 1\n",
        "dL_da1_relu = dL_da2_relu * da2_dz2_relu * w2\n",
        "da1_dz1_relu = relu_derivative(z1_relu)\n",
        "dL_dw1_relu = dL_da1_relu * da1_dz1_relu * x\n",
        "\n",
        "# Print results for ReLU\n",
        "print(\"\\nForward Pass (ReLU):\")\n",
        "print(f\"Layer 1 Output (a1): {a1_relu:.4f}\")\n",
        "print(f\"Layer 2 Output (a2): {a2_relu:.4f}\")\n",
        "print(f\"Predicted Output (y_pred): {y_pred_relu:.4f}\")\n",
        "print(f\"Loss: {loss_relu:.4f}\")\n",
        "\n",
        "print(\"\\nBackward Pass (Gradients for ReLU):\")\n",
        "print(f\"Gradient w.r.t. w3: {dL_dw3_relu:.4f}\")\n",
        "print(f\"Gradient w.r.t. w2: {dL_dw2_relu:.4f}\")\n",
        "print(f\"Gradient w.r.t. w1: {dL_dw1_relu:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\nComparison:\")\n",
        "print(\"Sigmoid Gradients:\")\n",
        "print(f\"Gradient w.r.t. w3: {dL_dw3_sigmoid:.4f}\")\n",
        "print(f\"Gradient w.r.t. w2: {dL_dw2_sigmoid:.4f}\")\n",
        "print(f\"Gradient w.r.t. w1: {dL_dw1_sigmoid:.4f}\")\n",
        "\n",
        "print(\"\\nReLU Gradients:\")\n",
        "print(f\"Gradient w.r.t. w3: {dL_dw3_relu:.4f}\")\n",
        "print(f\"Gradient w.r.t. w2: {dL_dw2_relu:.4f}\")\n",
        "print(f\"Gradient w.r.t. w1: {dL_dw1_relu:.4f}\")"
      ]
    }
  ]
}